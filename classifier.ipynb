{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from numpy import asarray, zeros\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "This step further cleans the text data by converting text to lowercase, removing punctuation and extra spaces.\n",
    "\n",
    "Certain stopwords affected the sentiment of the text and hence removing them would decrease the accuracy of the model. Furthermore, this project implements a neural network approach and thus\n",
    "1. directional words such as ['above', 'below', 'up', 'down', 'over', 'under']\n",
    "2. words that serves as a negation such as 'can't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                  MNTA Over 12.00            1\n",
       "4                                   OI  Over 21.37            1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('stock_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       " 1    3685\n",
       "-1    2106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of positive and negative sentiments\n",
    "df['Sentiment'].value_counts() # more positive than negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gareth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_words = ['above', 'below', 'up', 'down', 'over', 'under']\n",
    "neg_words = ['but', 'no', 'nor', 'not', 'don', \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "exception_words = dir_words + neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to process the sentences\n",
    "def process_text(text):\n",
    "    \n",
    "    cleaned = text.lower()\n",
    "\n",
    "    # Remove punctuations\n",
    "    cleaned = re.sub('[^a-zA-Z]', ' ', cleaned)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "\n",
    "    # Remove stopwords\n",
    "    '''\n",
    "    remove words that affect sentiment from the list of stopwords such as:\n",
    "    ['above', 'below', 'up', 'down', 'over', 'under', 'no', 'nor']\n",
    "    '''\n",
    "    stop_words = [word for word in stopwords.words('english') if word not in exception_words] \n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*') \n",
    "    cleaned = pattern.sub('', cleaned)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user afraid short amzn looking like near monopoly ebooks infrastructure service'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = \"user I'd be afraid to short AMZN - and they are looking like a near-monopoly in eBooks and infrastructure-as-a-service\"\n",
    "process_text(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to stem the sentences\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kicker watchlist xide tit soq pnk cpw bpz aj trade method method see prev post'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = 'kickers watchlist xide tit soq pnk cpw bpz aj trade method method see prev posts'\n",
    "stem_text(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kicker watchlist xide tit soq pnk cpw bpz aj t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user aap movi return fea geed indic trade year...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user afraid short amzn look like near monopoli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnta over</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oi over</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  kicker watchlist xide tit soq pnk cpw bpz aj t...          1\n",
       "1  user aap movi return fea geed indic trade year...          1\n",
       "2  user afraid short amzn look like near monopoli...          1\n",
       "3                                          mnta over          1\n",
       "4                                            oi over          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: stem_text(process_text(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>momentum come back etfc broke ma resist solid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>user gameplan shot today but like trend break ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vs invert head shoulder play well wasn abl cat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>red not readi break</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>user bac quick trade late but invest good entr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5779</th>\n",
       "      <td>investor lose rs lakh crore worst day market o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>tc share price jump no layoff dividend announc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>gold price slip below rs investor book profit ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>sharemarket live sensex day high up point nift...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>sensex nifti climb day high still up key facto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "14    momentum come back etfc broke ma resist solid ...          1\n",
       "16    user gameplan shot today but like trend break ...          1\n",
       "25    vs invert head shoulder play well wasn abl cat...          1\n",
       "26                                  red not readi break         -1\n",
       "28    user bac quick trade late but invest good entr...          1\n",
       "...                                                 ...        ...\n",
       "5779  investor lose rs lakh crore worst day market o...         -1\n",
       "5785  tc share price jump no layoff dividend announc...          1\n",
       "5787  gold price slip below rs investor book profit ...         -1\n",
       "5789  sharemarket live sensex day high up point nift...          1\n",
       "5790  sensex nifti climb day high still up key facto...          1\n",
       "\n",
       "[1458 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the exceptions affects the sentiment\n",
    "df.loc[df['Text'].str.contains(' | '.join(exception_words)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = df['Sentiment'].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word freq dict\n",
    "all_words = []\n",
    "\n",
    "for sentence in X:\n",
    "    words = sentence.split()\n",
    "    all_words.extend(words)\n",
    "\n",
    "freq_dist = FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'aap': 929, 'co': 711, 'http': 696, 'user': 648, 'short': 522, 'up': 440, 'day': 385, 'stock': 372, 'over': 352, 'today': 347, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of words with freq <= threshold\n",
    "'''\n",
    "Attempted tweaking the threshold but did not find any significant improvements\n",
    "'''\n",
    "threshold = 0 # threshold for low freq words \n",
    "low_freq_list = []\n",
    "\n",
    "for word, freq in freq_dist.items():\n",
    "    if freq <= threshold: \n",
    "        low_freq_list.append(word)\n",
    "\n",
    "low_freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove words with low freq\n",
    "def remove_low_freq(text):\n",
    "    words = text.split()\n",
    "    removed = [word for word in words if word not in low_freq_list]\n",
    "    return ' '.join(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       kicker watchlist xide tit soq pnk cpw bpz aj t...\n",
       "1       user aap movi return fea geed indic trade year...\n",
       "2       user afraid short amzn look like near monopoli...\n",
       "3                                               mnta over\n",
       "4                                                 oi over\n",
       "                              ...                        \n",
       "5786    industri bodi cii said discom like suffer net ...\n",
       "5787    gold price slip below rs investor book profit ...\n",
       "5788    worker bajaj auto agre wage cut period april t...\n",
       "5789    sharemarket live sensex day high up point nift...\n",
       "5790    sensex nifti climb day high still up key facto...\n",
       "Name: Text, Length: 5791, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.apply(lambda x: remove_low_freq(x))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4632,), (1159,), (4632,), (1159,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80 20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing\n",
    "\n",
    "convert every sentence into an sequences of indexes which represent the words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Create word to index dictionary\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sentence length is 22\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for i in X_train:\n",
    "    if len(i) > maxlen:\n",
    "        maxlen = len(i)\n",
    "for i in X_test:\n",
    "    if len(i) > maxlen:\n",
    "        maxlen = len(i)\n",
    "print(f'The maximum sentence length is {maxlen}')\n",
    "\n",
    "#maxlen = 50 # for glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aap': 1,\n",
       " 'co': 2,\n",
       " 'http': 3,\n",
       " 'user': 4,\n",
       " 'short': 5,\n",
       " 'up': 6,\n",
       " 'stock': 7,\n",
       " 'day': 8,\n",
       " 'today': 9,\n",
       " 'over': 10,\n",
       " 'like': 11,\n",
       " 'look': 12,\n",
       " 'volum': 13,\n",
       " 'market': 14,\n",
       " 'buy': 15,\n",
       " 'long': 16,\n",
       " 'but': 17,\n",
       " 'stop': 18,\n",
       " 'move': 19,\n",
       " 'watch': 20,\n",
       " 'go': 21,\n",
       " 'trade': 22,\n",
       " 'break': 23,\n",
       " 'not': 24,\n",
       " 'good': 25,\n",
       " 'nice': 26,\n",
       " 'goog': 27,\n",
       " 'high': 28,\n",
       " 'new': 29,\n",
       " 'still': 30,\n",
       " 'abov': 31,\n",
       " 'back': 32,\n",
       " 'down': 33,\n",
       " 'close': 34,\n",
       " 'bac': 35,\n",
       " 'time': 36,\n",
       " 'next': 37,\n",
       " 'week': 38,\n",
       " 'see': 39,\n",
       " 'coronaviru': 40,\n",
       " 'call': 41,\n",
       " 'get': 42,\n",
       " 'ong': 43,\n",
       " 'price': 44,\n",
       " 'one': 45,\n",
       " 'posit': 46,\n",
       " 'higher': 47,\n",
       " 'hold': 48,\n",
       " 'no': 49,\n",
       " 'point': 50,\n",
       " 'sell': 51,\n",
       " 'share': 52,\n",
       " 'triangl': 53,\n",
       " 'breakout': 54,\n",
       " 'could': 55,\n",
       " 'open': 56,\n",
       " 'take': 57,\n",
       " 'weekli': 58,\n",
       " 'sensex': 59,\n",
       " 'target': 60,\n",
       " 'big': 61,\n",
       " 'year': 62,\n",
       " 'come': 63,\n",
       " 'nifti': 64,\n",
       " 'support': 65,\n",
       " 'bullish': 66,\n",
       " 'p': 67,\n",
       " 'gap': 68,\n",
       " 'low': 69,\n",
       " 'last': 70,\n",
       " 'lower': 71,\n",
       " 'nfx': 72,\n",
       " 'green': 73,\n",
       " 'amzn': 74,\n",
       " 'earn': 75,\n",
       " 'put': 76,\n",
       " 'bank': 77,\n",
       " 'make': 78,\n",
       " 'start': 79,\n",
       " 'need': 80,\n",
       " 'f': 81,\n",
       " 'continu': 82,\n",
       " 'line': 83,\n",
       " 'say': 84,\n",
       " 'think': 85,\n",
       " 'rt': 86,\n",
       " 'under': 87,\n",
       " 'profit': 88,\n",
       " 'resist': 89,\n",
       " 'below': 90,\n",
       " 'c': 91,\n",
       " 'strong': 92,\n",
       " 'term': 93,\n",
       " 'flag': 94,\n",
       " 'may': 95,\n",
       " 'sinc': 96,\n",
       " 'trend': 97,\n",
       " 'amid': 98,\n",
       " 'w': 99,\n",
       " 'ad': 100,\n",
       " 'b': 101,\n",
       " 'k': 102,\n",
       " 'would': 103,\n",
       " 'cash': 104,\n",
       " 'spi': 105,\n",
       " 'level': 106,\n",
       " 'first': 107,\n",
       " 'well': 108,\n",
       " 'u': 109,\n",
       " 'hit': 110,\n",
       " 'list': 111,\n",
       " 'ddd': 112,\n",
       " 'expect': 113,\n",
       " 'want': 114,\n",
       " 'yesterday': 115,\n",
       " 'post': 116,\n",
       " 'bounc': 117,\n",
       " 'ma': 118,\n",
       " 'tomorrow': 119,\n",
       " 'option': 120,\n",
       " 'ook': 121,\n",
       " 'month': 122,\n",
       " 'anoth': 123,\n",
       " 'set': 124,\n",
       " 'appl': 125,\n",
       " 'fall': 126,\n",
       " 'gs': 127,\n",
       " 'investor': 128,\n",
       " 'great': 129,\n",
       " 'tri': 130,\n",
       " 'daili': 131,\n",
       " 'keep': 132,\n",
       " 'play': 133,\n",
       " 'compani': 134,\n",
       " 'v': 135,\n",
       " 'x': 136,\n",
       " 'top': 137,\n",
       " 'money': 138,\n",
       " 'run': 139,\n",
       " 'bull': 140,\n",
       " 'full': 141,\n",
       " 'q': 142,\n",
       " 'gain': 143,\n",
       " 'ed': 144,\n",
       " 'end': 145,\n",
       " 'drop': 146,\n",
       " 'entri': 147,\n",
       " 'street': 148,\n",
       " 'bought': 149,\n",
       " 'don': 150,\n",
       " 'million': 151,\n",
       " 'right': 152,\n",
       " 'imo': 153,\n",
       " 'global': 154,\n",
       " 'soon': 155,\n",
       " 'much': 156,\n",
       " 'fund': 157,\n",
       " 'possibl': 158,\n",
       " 'pay': 159,\n",
       " 'pullback': 160,\n",
       " 'follow': 161,\n",
       " 'base': 162,\n",
       " 'wait': 163,\n",
       " 'beat': 164,\n",
       " 'oil': 165,\n",
       " 'march': 166,\n",
       " 'red': 167,\n",
       " 'interest': 168,\n",
       " 'work': 169,\n",
       " 'around': 170,\n",
       " 'consolid': 171,\n",
       " 'way': 172,\n",
       " 'report': 173,\n",
       " 'news': 174,\n",
       " 'e': 175,\n",
       " 'pattern': 176,\n",
       " 'bottom': 177,\n",
       " 'znga': 178,\n",
       " 'fill': 179,\n",
       " 'know': 180,\n",
       " 'test': 181,\n",
       " 'sma': 182,\n",
       " 'cat': 183,\n",
       " 'show': 184,\n",
       " 'also': 185,\n",
       " 'bearish': 186,\n",
       " 'littl': 187,\n",
       " 'nkd': 188,\n",
       " 'growth': 189,\n",
       " 'head': 190,\n",
       " 'friday': 191,\n",
       " 'loss': 192,\n",
       " 'tgt': 193,\n",
       " 'small': 194,\n",
       " 'action': 195,\n",
       " 'realli': 196,\n",
       " 'vol': 197,\n",
       " 'setup': 198,\n",
       " 'scale': 199,\n",
       " 'even': 200,\n",
       " 'upsid': 201,\n",
       " 'sold': 202,\n",
       " 'sale': 203,\n",
       " 'h': 204,\n",
       " 'rise': 205,\n",
       " 'spx': 206,\n",
       " 'min': 207,\n",
       " 'confirm': 208,\n",
       " 'vs': 209,\n",
       " 'macd': 210,\n",
       " 'better': 211,\n",
       " 'trader': 212,\n",
       " 'hope': 213,\n",
       " 'shot': 214,\n",
       " 'bet': 215,\n",
       " 'feb': 216,\n",
       " 'swing': 217,\n",
       " 'bear': 218,\n",
       " 'goe': 219,\n",
       " 'pop': 220,\n",
       " 'near': 221,\n",
       " 'lot': 222,\n",
       " 'ng': 223,\n",
       " 'st': 224,\n",
       " 'es': 225,\n",
       " 'crisi': 226,\n",
       " 'cut': 227,\n",
       " 'fb': 228,\n",
       " 'fs': 229,\n",
       " 'got': 230,\n",
       " 'aapl': 231,\n",
       " 'morn': 232,\n",
       " 'jan': 233,\n",
       " 'busi': 234,\n",
       " 'us': 235,\n",
       " 'averag': 236,\n",
       " 'weak': 237,\n",
       " 'bid': 238,\n",
       " 'ssi': 239,\n",
       " 'cover': 240,\n",
       " 'cross': 241,\n",
       " 'jcp': 242,\n",
       " 'pandem': 243,\n",
       " 'th': 244,\n",
       " 'key': 245,\n",
       " 'video': 246,\n",
       " 'net': 247,\n",
       " 'pick': 248,\n",
       " 'jpm': 249,\n",
       " 'dollar': 250,\n",
       " 'cf': 251,\n",
       " 'mani': 252,\n",
       " 'heard': 253,\n",
       " 'ema': 254,\n",
       " 'love': 255,\n",
       " 'pull': 256,\n",
       " 'give': 257,\n",
       " 'broke': 258,\n",
       " 'add': 259,\n",
       " 'major': 260,\n",
       " 'might': 261,\n",
       " 'imho': 262,\n",
       " 'two': 263,\n",
       " 'readi': 264,\n",
       " 'recent': 265,\n",
       " 'de': 266,\n",
       " 'china': 267,\n",
       " 'let': 268,\n",
       " 'invest': 269,\n",
       " 'msft': 270,\n",
       " 'si': 271,\n",
       " 'ralli': 272,\n",
       " 'turn': 273,\n",
       " 'csn': 274,\n",
       " 'best': 275,\n",
       " 'thing': 276,\n",
       " 'huge': 277,\n",
       " 'trigger': 278,\n",
       " 'peopl': 279,\n",
       " 'govern': 280,\n",
       " 'stay': 281,\n",
       " 'made': 282,\n",
       " 'world': 283,\n",
       " 'nvda': 284,\n",
       " 'miss': 285,\n",
       " 'declin': 286,\n",
       " 'gd': 287,\n",
       " 'lockdown': 288,\n",
       " 'g': 289,\n",
       " 'area': 290,\n",
       " 'rang': 291,\n",
       " 'gmc': 292,\n",
       " 'trail': 293,\n",
       " 'signal': 294,\n",
       " 'monthli': 295,\n",
       " 'billion': 296,\n",
       " 'idea': 297,\n",
       " 'pdate': 298,\n",
       " 'clear': 299,\n",
       " 'push': 300,\n",
       " 'economi': 301,\n",
       " 'financi': 302,\n",
       " 'indic': 303,\n",
       " 'oper': 304,\n",
       " 'cost': 305,\n",
       " 'bbi': 306,\n",
       " 'eye': 307,\n",
       " 'bar': 308,\n",
       " 'channel': 309,\n",
       " 'jump': 310,\n",
       " 'minut': 311,\n",
       " 'fed': 312,\n",
       " 'done': 313,\n",
       " 'technic': 314,\n",
       " 'increas': 315,\n",
       " 'revers': 316,\n",
       " 'intraday': 317,\n",
       " 'probabl': 318,\n",
       " 'hpq': 319,\n",
       " 'hour': 320,\n",
       " 'r': 321,\n",
       " 'yhoo': 322,\n",
       " 'includ': 323,\n",
       " 'hedg': 324,\n",
       " 'rbi': 325,\n",
       " 'earli': 326,\n",
       " 'hard': 327,\n",
       " 'via': 328,\n",
       " 'dividend': 329,\n",
       " 'dip': 330,\n",
       " 'mayb': 331,\n",
       " 'data': 332,\n",
       " 'cup': 333,\n",
       " 'handl': 334,\n",
       " 'po': 335,\n",
       " 'far': 336,\n",
       " 'mcp': 337,\n",
       " 'ish': 338,\n",
       " 'buyer': 339,\n",
       " 'n': 340,\n",
       " 'ave': 341,\n",
       " 'away': 342,\n",
       " 'spw': 343,\n",
       " 'job': 344,\n",
       " 'check': 345,\n",
       " 'rs': 346,\n",
       " 'valu': 347,\n",
       " 'yr': 348,\n",
       " 'use': 349,\n",
       " 'bond': 350,\n",
       " 'neg': 351,\n",
       " 'surg': 352,\n",
       " 'worst': 353,\n",
       " 'uptrend': 354,\n",
       " 'someth': 355,\n",
       " 'app': 356,\n",
       " 'ko': 357,\n",
       " 'industri': 358,\n",
       " 'easi': 359,\n",
       " 'prior': 360,\n",
       " 'offer': 361,\n",
       " 'almost': 362,\n",
       " 'solid': 363,\n",
       " 'game': 364,\n",
       " 'thru': 365,\n",
       " 'quarter': 366,\n",
       " 'potenti': 367,\n",
       " 'fcx': 368,\n",
       " 'sv': 369,\n",
       " 'chk': 370,\n",
       " 'demand': 371,\n",
       " 'took': 372,\n",
       " 'cheap': 373,\n",
       " 'past': 374,\n",
       " 'bit': 375,\n",
       " 'wall': 376,\n",
       " 'candl': 377,\n",
       " 'fail': 378,\n",
       " 'fib': 379,\n",
       " 'monday': 380,\n",
       " 'return': 381,\n",
       " 'dump': 382,\n",
       " 'ep': 383,\n",
       " 'swi': 384,\n",
       " 'seem': 385,\n",
       " 'april': 386,\n",
       " 'mo': 387,\n",
       " 'gpn': 388,\n",
       " 'final': 389,\n",
       " 'flow': 390,\n",
       " 'gold': 391,\n",
       " 'tight': 392,\n",
       " 'qqq': 393,\n",
       " 'fast': 394,\n",
       " 'wk': 395,\n",
       " 'watchlist': 396,\n",
       " 'squeez': 397,\n",
       " 'cs': 398,\n",
       " 'pcn': 399,\n",
       " 'help': 400,\n",
       " 'current': 401,\n",
       " 'india': 402,\n",
       " 'ebay': 403,\n",
       " 'late': 404,\n",
       " 'econom': 405,\n",
       " 'plan': 406,\n",
       " 'pt': 407,\n",
       " 'win': 408,\n",
       " 'mar': 409,\n",
       " 'ibm': 410,\n",
       " 'everyon': 411,\n",
       " 'second': 412,\n",
       " 'everi': 413,\n",
       " 'talk': 414,\n",
       " 'wmt': 415,\n",
       " 'risk': 416,\n",
       " 'sure': 417,\n",
       " 'ist': 418,\n",
       " 'build': 419,\n",
       " 'doubl': 420,\n",
       " 'form': 421,\n",
       " 'note': 422,\n",
       " 'deck': 423,\n",
       " 'worth': 424,\n",
       " 'rate': 425,\n",
       " 'invn': 426,\n",
       " 'book': 427,\n",
       " 'bad': 428,\n",
       " 'updat': 429,\n",
       " 'ago': 430,\n",
       " 'happen': 431,\n",
       " 'qcom': 432,\n",
       " 'vxi': 433,\n",
       " 'pm': 434,\n",
       " 'wow': 435,\n",
       " 'anyon': 436,\n",
       " 'ye': 437,\n",
       " 'lead': 438,\n",
       " 'due': 439,\n",
       " 'feel': 440,\n",
       " 'hot': 441,\n",
       " 'oi': 442,\n",
       " 'pretti': 443,\n",
       " 'side': 444,\n",
       " 'ms': 445,\n",
       " 'mean': 446,\n",
       " 'volatil': 447,\n",
       " 'home': 448,\n",
       " 'record': 449,\n",
       " 'z': 450,\n",
       " 'said': 451,\n",
       " 'vng': 452,\n",
       " 'rest': 453,\n",
       " 'shd': 454,\n",
       " 'manag': 455,\n",
       " 'rais': 456,\n",
       " 'quick': 457,\n",
       " 'trendlin': 458,\n",
       " 'avg': 459,\n",
       " 'slow': 460,\n",
       " 'alway': 461,\n",
       " 'sign': 462,\n",
       " 'overnight': 463,\n",
       " 'futur': 464,\n",
       " 'step': 465,\n",
       " 'alreadi': 466,\n",
       " 'eod': 467,\n",
       " 'doesn': 468,\n",
       " 'lol': 469,\n",
       " 'never': 470,\n",
       " 'won': 471,\n",
       " 'xco': 472,\n",
       " 'till': 473,\n",
       " 'tell': 474,\n",
       " 'ow': 475,\n",
       " 'cee': 476,\n",
       " 'cst': 477,\n",
       " 'hod': 478,\n",
       " 'remain': 479,\n",
       " 'portfolio': 480,\n",
       " 'swhc': 481,\n",
       " 'downsid': 482,\n",
       " 'fear': 483,\n",
       " 'amp': 484,\n",
       " 'fio': 485,\n",
       " 'tc': 486,\n",
       " 'momentum': 487,\n",
       " 'split': 488,\n",
       " 'margin': 489,\n",
       " 'crore': 490,\n",
       " 'sbx': 491,\n",
       " 'roll': 492,\n",
       " 'account': 493,\n",
       " 'enter': 494,\n",
       " 'deal': 495,\n",
       " 'marketupd': 496,\n",
       " 'announc': 497,\n",
       " 'mon': 498,\n",
       " 'larg': 499,\n",
       " 'ever': 500,\n",
       " 'touch': 501,\n",
       " 'per': 502,\n",
       " 'analysi': 503,\n",
       " 'pre': 504,\n",
       " 'expir': 505,\n",
       " 'went': 506,\n",
       " 'ana': 507,\n",
       " 'spread': 508,\n",
       " 'biggest': 509,\n",
       " 'da': 510,\n",
       " 'mtg': 511,\n",
       " 'heavi': 512,\n",
       " 'loan': 513,\n",
       " 'half': 514,\n",
       " 'size': 515,\n",
       " 'number': 516,\n",
       " 'ceo': 517,\n",
       " 'cent': 518,\n",
       " 'wfc': 519,\n",
       " 'addit': 520,\n",
       " 'iphon': 521,\n",
       " 'debt': 522,\n",
       " 'oversold': 523,\n",
       " 'insid': 524,\n",
       " 'shoulder': 525,\n",
       " 'reason': 526,\n",
       " 'catch': 527,\n",
       " 'oc': 528,\n",
       " 'free': 529,\n",
       " 'rebound': 530,\n",
       " 'cap': 531,\n",
       " 'though': 532,\n",
       " 'imm': 533,\n",
       " 'leg': 534,\n",
       " 'consum': 535,\n",
       " 'inflat': 536,\n",
       " 'trap': 537,\n",
       " 'despit': 538,\n",
       " 'product': 539,\n",
       " 'sorri': 540,\n",
       " 'oh': 541,\n",
       " 'apo': 542,\n",
       " 'gevo': 543,\n",
       " 'care': 544,\n",
       " 'dma': 545,\n",
       " 'result': 546,\n",
       " 'chang': 547,\n",
       " 'samsung': 548,\n",
       " 'aa': 549,\n",
       " 'dow': 550,\n",
       " 'seller': 551,\n",
       " 'gae': 552,\n",
       " 'american': 553,\n",
       " 'yet': 554,\n",
       " 'retail': 555,\n",
       " 'bring': 556,\n",
       " 'real': 557,\n",
       " 'purchas': 558,\n",
       " 'thursday': 559,\n",
       " 'dndn': 560,\n",
       " 'fli': 561,\n",
       " 'thank': 562,\n",
       " 'name': 563,\n",
       " 'thought': 564,\n",
       " 'suggest': 565,\n",
       " 'correct': 566,\n",
       " 'estim': 567,\n",
       " 'left': 568,\n",
       " 'lose': 569,\n",
       " 'aig': 570,\n",
       " 'wrap': 571,\n",
       " 'fire': 572,\n",
       " 'mkt': 573,\n",
       " 'covid': 574,\n",
       " 'agre': 575,\n",
       " 'sector': 576,\n",
       " 'extend': 577,\n",
       " 'ot': 578,\n",
       " 'analyst': 579,\n",
       " 'other': 580,\n",
       " 'downgrad': 581,\n",
       " 'broken': 582,\n",
       " 'store': 583,\n",
       " 'ha': 584,\n",
       " 'plung': 585,\n",
       " 'mark': 586,\n",
       " 'gmx': 587,\n",
       " 'j': 588,\n",
       " 'januari': 589,\n",
       " 'mention': 590,\n",
       " 'bvsn': 591,\n",
       " 'ahead': 592,\n",
       " 'europ': 593,\n",
       " 'equiti': 594,\n",
       " 'sd': 595,\n",
       " 'beauti': 596,\n",
       " 'patienc': 597,\n",
       " 'ove': 598,\n",
       " 'kbh': 599,\n",
       " 'ft': 600,\n",
       " 'dead': 601,\n",
       " 'less': 602,\n",
       " 'place': 603,\n",
       " 'find': 604,\n",
       " 'cloud': 605,\n",
       " 'face': 606,\n",
       " 'l': 607,\n",
       " 'dvax': 608,\n",
       " 'saudi': 609,\n",
       " 'night': 610,\n",
       " 'ga': 611,\n",
       " 'overbought': 612,\n",
       " 'everyth': 613,\n",
       " 'tonight': 614,\n",
       " 'ask': 615,\n",
       " 'intc': 616,\n",
       " 'struggl': 617,\n",
       " 'ike': 618,\n",
       " 'act': 619,\n",
       " 'affi': 620,\n",
       " 'stori': 621,\n",
       " 'gonna': 622,\n",
       " 'en': 623,\n",
       " 'part': 624,\n",
       " 'downtrend': 625,\n",
       " 'appear': 626,\n",
       " 'climb': 627,\n",
       " 'div': 628,\n",
       " 'rupe': 629,\n",
       " 'guy': 630,\n",
       " 'phone': 631,\n",
       " 'without': 632,\n",
       " 'state': 633,\n",
       " 'hate': 634,\n",
       " 'believ': 635,\n",
       " 'compq': 636,\n",
       " 'dia': 637,\n",
       " 'rip': 638,\n",
       " 'dec': 639,\n",
       " 'power': 640,\n",
       " 'upward': 641,\n",
       " 'grow': 642,\n",
       " 'earlier': 643,\n",
       " 'ca': 644,\n",
       " 'dnkn': 645,\n",
       " 'straight': 646,\n",
       " 'cm': 647,\n",
       " 'wsj': 648,\n",
       " 'surpris': 649,\n",
       " 'float': 650,\n",
       " 'nearli': 651,\n",
       " 'bk': 652,\n",
       " 'diverg': 653,\n",
       " 'didn': 654,\n",
       " 'revenu': 655,\n",
       " 'noth': 656,\n",
       " 'vmw': 657,\n",
       " 'largest': 658,\n",
       " 'retest': 659,\n",
       " 'consid': 660,\n",
       " 'old': 661,\n",
       " 'qco': 662,\n",
       " 'tv': 663,\n",
       " 'firm': 664,\n",
       " 'pivot': 665,\n",
       " 'contract': 666,\n",
       " 'wonder': 667,\n",
       " 'airlin': 668,\n",
       " 'blog': 669,\n",
       " 'employe': 670,\n",
       " 'three': 671,\n",
       " 'ipad': 672,\n",
       " 'group': 673,\n",
       " 'either': 674,\n",
       " 'read': 675,\n",
       " 'countri': 676,\n",
       " 'begin': 677,\n",
       " 'pphm': 678,\n",
       " 'impact': 679,\n",
       " 'initi': 680,\n",
       " 'tech': 681,\n",
       " 'wynn': 682,\n",
       " 'sca': 683,\n",
       " 'eas': 684,\n",
       " 'sa': 685,\n",
       " 'februari': 686,\n",
       " 'se': 687,\n",
       " 'pressur': 688,\n",
       " 'ead': 689,\n",
       " 'problem': 690,\n",
       " 'exp': 691,\n",
       " 'ax': 692,\n",
       " 'accumul': 693,\n",
       " 'someon': 694,\n",
       " 'stream': 695,\n",
       " 'perform': 696,\n",
       " 'solar': 697,\n",
       " 'enough': 698,\n",
       " 'sharehold': 699,\n",
       " 'alert': 700,\n",
       " 'least': 701,\n",
       " 'sound': 702,\n",
       " 'gdx': 703,\n",
       " 'haven': 704,\n",
       " 'smart': 705,\n",
       " 'yep': 706,\n",
       " 'kwk': 707,\n",
       " 'hammer': 708,\n",
       " 'corpor': 709,\n",
       " 'onlin': 710,\n",
       " 'spot': 711,\n",
       " 'creat': 712,\n",
       " 'held': 713,\n",
       " 'crash': 714,\n",
       " 'lost': 715,\n",
       " 'vhc': 716,\n",
       " 'upgrad': 717,\n",
       " 'releas': 718,\n",
       " 'sk': 719,\n",
       " 'winner': 720,\n",
       " 'toward': 721,\n",
       " 'coh': 722,\n",
       " 'governor': 723,\n",
       " 'crude': 724,\n",
       " 'ratio': 725,\n",
       " 'total': 726,\n",
       " 'seen': 727,\n",
       " 'financ': 728,\n",
       " 'hd': 729,\n",
       " 'px': 730,\n",
       " 'massiv': 731,\n",
       " 'suppli': 732,\n",
       " 'tsa': 733,\n",
       " 'slump': 734,\n",
       " 'block': 735,\n",
       " 'pois': 736,\n",
       " 'card': 737,\n",
       " 'ocn': 738,\n",
       " 'lag': 739,\n",
       " 'tap': 740,\n",
       " 'within': 741,\n",
       " 'capit': 742,\n",
       " 'gdp': 743,\n",
       " 'premarket': 744,\n",
       " 'feder': 745,\n",
       " 'cmg': 746,\n",
       " 'central': 747,\n",
       " 'ride': 748,\n",
       " 'radar': 749,\n",
       " 'fight': 750,\n",
       " 'box': 751,\n",
       " 'must': 752,\n",
       " 'chase': 753,\n",
       " 'claim': 754,\n",
       " 'zone': 755,\n",
       " 'amazon': 756,\n",
       " 'ctic': 757,\n",
       " 'panic': 758,\n",
       " 'rev': 759,\n",
       " 'cox': 760,\n",
       " 'matter': 761,\n",
       " 'sit': 762,\n",
       " 'nke': 763,\n",
       " 'nx': 764,\n",
       " 'byd': 765,\n",
       " 'vv': 766,\n",
       " 'improv': 767,\n",
       " 'csco': 768,\n",
       " 'borrow': 769,\n",
       " 'liquid': 770,\n",
       " 'upper': 771,\n",
       " 'anad': 772,\n",
       " 'di': 773,\n",
       " 'servic': 774,\n",
       " 'balanc': 775,\n",
       " 'later': 776,\n",
       " 'selloff': 777,\n",
       " 'along': 778,\n",
       " 'die': 779,\n",
       " 'spike': 780,\n",
       " 'momo': 781,\n",
       " 'ignor': 782,\n",
       " 'usual': 783,\n",
       " 'aid': 784,\n",
       " 'dk': 785,\n",
       " 'forecast': 786,\n",
       " 'cook': 787,\n",
       " 'south': 788,\n",
       " 'came': 789,\n",
       " 'ok': 790,\n",
       " 'wors': 791,\n",
       " 'settl': 792,\n",
       " 'stoploss': 793,\n",
       " 'reduc': 794,\n",
       " 'reserv': 795,\n",
       " 'activ': 796,\n",
       " 'save': 797,\n",
       " 'crack': 798,\n",
       " 'measur': 799,\n",
       " 'decent': 800,\n",
       " 'light': 801,\n",
       " 'resum': 802,\n",
       " 'he': 803,\n",
       " 'nd': 804,\n",
       " 'ndx': 805,\n",
       " 'slide': 806,\n",
       " 'ascend': 807,\n",
       " 'health': 808,\n",
       " 'pin': 809,\n",
       " 'view': 810,\n",
       " 'index': 811,\n",
       " 'order': 812,\n",
       " 'limit': 813,\n",
       " 'boost': 814,\n",
       " 'ec': 815,\n",
       " 'social': 816,\n",
       " 'isn': 817,\n",
       " 'ceg': 818,\n",
       " 'delta': 819,\n",
       " 'compar': 820,\n",
       " 'longer': 821,\n",
       " 'recess': 822,\n",
       " 'wt': 823,\n",
       " 'breakdown': 824,\n",
       " 'fresh': 825,\n",
       " 'wave': 826,\n",
       " 'pump': 827,\n",
       " 'drive': 828,\n",
       " 'war': 829,\n",
       " 'cog': 830,\n",
       " 'saw': 831,\n",
       " 'payment': 832,\n",
       " 'cen': 833,\n",
       " 'gener': 834,\n",
       " 'opex': 835,\n",
       " 'bot': 836,\n",
       " 'figur': 837,\n",
       " 'fade': 838,\n",
       " 'told': 839,\n",
       " 'chanc': 840,\n",
       " 'opportun': 841,\n",
       " 'pleas': 842,\n",
       " 'sto': 843,\n",
       " 'public': 844,\n",
       " 'model': 845,\n",
       " 'previou': 846,\n",
       " 'chines': 847,\n",
       " 'mobil': 848,\n",
       " 'space': 849,\n",
       " 'secur': 850,\n",
       " 'ngt': 851,\n",
       " 'ppl': 852,\n",
       " 'advanc': 853,\n",
       " 'edg': 854,\n",
       " 'vz': 855,\n",
       " 'fnfg': 856,\n",
       " 'benefit': 857,\n",
       " 'reclaim': 858,\n",
       " 'oad': 859,\n",
       " 'dt': 860,\n",
       " 'stake': 861,\n",
       " 'shaktikanta': 862,\n",
       " 'anybodi': 863,\n",
       " 'hang': 864,\n",
       " 'inc': 865,\n",
       " 'axp': 866,\n",
       " 'forc': 867,\n",
       " 'strength': 868,\n",
       " 'glass': 869,\n",
       " 'peix': 870,\n",
       " 'tweet': 871,\n",
       " 'patient': 872,\n",
       " 'gme': 873,\n",
       " 'guess': 874,\n",
       " 'issu': 875,\n",
       " 'prefer': 876,\n",
       " 'wen': 877,\n",
       " 'foreign': 878,\n",
       " 'becom': 879,\n",
       " 'explain': 880,\n",
       " 'boom': 881,\n",
       " 'kcg': 882,\n",
       " 'retrac': 883,\n",
       " 'tuesday': 884,\n",
       " 'weekend': 885,\n",
       " 'leader': 886,\n",
       " 'actual': 887,\n",
       " 'els': 888,\n",
       " 'notic': 889,\n",
       " 'mind': 890,\n",
       " 'ay': 891,\n",
       " 'arabia': 892,\n",
       " 'einhorn': 893,\n",
       " 'dont': 894,\n",
       " 'door': 895,\n",
       " 'nov': 896,\n",
       " 'remind': 897,\n",
       " 'decis': 898,\n",
       " 'whole': 899,\n",
       " 'hf': 900,\n",
       " 'jobless': 901,\n",
       " 'bodi': 902,\n",
       " 'third': 903,\n",
       " 'apc': 904,\n",
       " 'damag': 905,\n",
       " 'fundament': 906,\n",
       " 'reward': 907,\n",
       " 'gt': 908,\n",
       " 'extrem': 909,\n",
       " 'session': 910,\n",
       " 'awesom': 911,\n",
       " 'hand': 912,\n",
       " 'googl': 913,\n",
       " 'vome': 914,\n",
       " 'morgan': 915,\n",
       " 'mako': 916,\n",
       " 'russia': 917,\n",
       " 'rd': 918,\n",
       " 'provid': 919,\n",
       " 'goldman': 920,\n",
       " 'wsjheard': 921,\n",
       " 'progress': 922,\n",
       " 'alpha': 923,\n",
       " 'import': 924,\n",
       " 'lakh': 925,\n",
       " 'descend': 926,\n",
       " 'csod': 927,\n",
       " 'ugli': 928,\n",
       " 'mgm': 929,\n",
       " 'viru': 930,\n",
       " 'gm': 931,\n",
       " 'im': 932,\n",
       " 'zc': 933,\n",
       " 'rememb': 934,\n",
       " 'tax': 935,\n",
       " 'trump': 936,\n",
       " 'research': 937,\n",
       " 'period': 938,\n",
       " 'et': 939,\n",
       " 'kex': 940,\n",
       " 'comment': 941,\n",
       " 'innov': 942,\n",
       " 'manufactur': 943,\n",
       " 'coil': 944,\n",
       " 'credit': 945,\n",
       " 'attack': 946,\n",
       " 'crazi': 947,\n",
       " 'dn': 948,\n",
       " 'system': 949,\n",
       " 'mini': 950,\n",
       " 'produc': 951,\n",
       " 'print': 952,\n",
       " 'lift': 953,\n",
       " 'magic': 954,\n",
       " 'tade': 955,\n",
       " 'ts': 956,\n",
       " 'softwar': 957,\n",
       " 'eadi': 958,\n",
       " 'technolog': 959,\n",
       " 'approach': 960,\n",
       " 'ba': 961,\n",
       " 'caus': 962,\n",
       " 'unit': 963,\n",
       " 'xf': 964,\n",
       " 'frame': 965,\n",
       " 'hek': 966,\n",
       " 'press': 967,\n",
       " 'confer': 968,\n",
       " 'coupl': 969,\n",
       " 'anyth': 970,\n",
       " 'instead': 971,\n",
       " 'wfm': 972,\n",
       " 'dd': 973,\n",
       " 'vxx': 974,\n",
       " 'io': 975,\n",
       " 'exposur': 976,\n",
       " 'sqnm': 977,\n",
       " 'finish': 978,\n",
       " 'buffett': 979,\n",
       " 'obv': 980,\n",
       " 'fun': 981,\n",
       " 'worri': 982,\n",
       " 'reaction': 983,\n",
       " 'histor': 984,\n",
       " 'fix': 985,\n",
       " 'intact': 986,\n",
       " 'mov': 987,\n",
       " 'scco': 988,\n",
       " 'articl': 989,\n",
       " 'project': 990,\n",
       " 'spend': 991,\n",
       " 'dot': 992,\n",
       " 'media': 993,\n",
       " 'yield': 994,\n",
       " 'write': 995,\n",
       " 'output': 996,\n",
       " 'man': 997,\n",
       " 'strike': 998,\n",
       " 'case': 999,\n",
       " 'troubl': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6409"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of unique words\n",
    "vocab_size = len(tokenizer.word_index) #+ 1 # for glove\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill sentences with len < maxlen with 0s\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embeddings_dict = dict()\\nglove_file = open(\\'glove_files/glove.6B.50d.txt\\', encoding=\"utf8\")\\n\\nfor line in glove_file:\\n    values = line.split()\\n    word = values[0]\\n    vector = asarray(values[1:], dtype=\\'float32\\')\\n    embeddings_dict[word] = vector\\nglove_file.close()\\n\\nembedding_matrix = zeros((vocab_size, 50))\\nfor word, index in tokenizer.word_index.items():\\n    embedding_vector = embeddings_dict.get(word)\\n    if embedding_vector is not None:\\n        embedding_matrix[index] = embedding_vector\\n\\nembedding_matrix.shape'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GloVe Embeddings\n",
    "'''embeddings_dict = dict()\n",
    "glove_file = open('glove_files/glove.6B.50d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = vector\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4632, 22), (1159, 22))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape = (num of rows, maxlen)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "An LSTM is used as the words are treated as 'timesteps' where each word affects subsequent inputs.\n",
    "\n",
    "Avoided overfitting by reducing epochs and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARS\n",
    "EPOCHS = 4 #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 10:33:27.966234: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-12-14 10:33:27.966261: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-12-14 10:33:27.966266: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-12-14 10:33:27.966299: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-14 10:33:27.966313: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 22, 32)            205088    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229985 (898.38 KB)\n",
      "Trainable params: 229985 (898.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = keras.Sequential([\n",
    "\n",
    "    #keras.layers.Embedding(input_dim=vocab_size, output_dim=50, weights=[embedding_matrix], input_length=maxlen), # GloVe embeddings\n",
    "    keras.layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=maxlen),\n",
    "    keras.layers.LSTM(units=64, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid') # softmax has higher accuracy but theoretically sigmoid is better for binary classification\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 10:33:28.676504: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 61s 522ms/step - loss: 0.6417 - accuracy: 0.6448 - val_loss: 0.5480 - val_accuracy: 0.6882\n",
      "Epoch 2/4\n",
      "116/116 [==============================] - 58s 495ms/step - loss: 0.4366 - accuracy: 0.8154 - val_loss: 0.4733 - val_accuracy: 0.7638\n",
      "Epoch 3/4\n",
      "116/116 [==============================] - 60s 514ms/step - loss: 0.2686 - accuracy: 0.8910 - val_loss: 0.5475 - val_accuracy: 0.7573\n",
      "Epoch 4/4\n",
      "116/116 [==============================] - 73s 608ms/step - loss: 0.1773 - accuracy: 0.9379 - val_loss: 0.6752 - val_accuracy: 0.7627\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 2s 58ms/step - loss: 0.6057 - accuracy: 0.7903\n",
      "Test accuracy: 0.7903364896774292\n"
     ]
    }
   ],
   "source": [
    "test_eval = model.evaluate(X_test, y_test)\n",
    "test_loss, test_acc = test_eval[0], test_eval[1]\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('final.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
