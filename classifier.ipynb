{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from numpy import asarray, zeros\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "This step further cleans the text data by converting text to lowercase, removing punctuation and numbers.\n",
    "\n",
    "Certain stopwords affected the sentiment of the text and hence removing them would decrease the accuracy of the model.\n",
    "1. directional words such as ['above', 'below', 'up', 'down', 'over', 'under']\n",
    "2. words that serves as a negation such as 'can't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                  MNTA Over 12.00            1\n",
       "4                                   OI  Over 21.37            1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('stock_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       " 1    3685\n",
       "-1    2106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of positive and negative sentiments\n",
    "df['Sentiment'].value_counts() # more positive than negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gareth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_words = ['above', 'below', 'up', 'down', 'over', 'under']\n",
    "neg_words = ['but', 'no', 'nor', 'not', 'don', \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "exception_words = dir_words + neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to process the sentences\n",
    "def process_text(text):\n",
    "    \n",
    "    cleaned = text.lower()\n",
    "\n",
    "    # Remove punctuations\n",
    "    cleaned = re.sub('[^a-zA-Z]', ' ', cleaned)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "\n",
    "    # Remove stopwords\n",
    "    '''\n",
    "    remove words that affect sentiment from the list of stopwords such as:\n",
    "    ['above', 'below', 'up', 'down', 'over', 'under', 'no', 'nor']\n",
    "    '''\n",
    "    stop_words = [word for word in stopwords.words('english') if word not in exception_words] \n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*') \n",
    "    cleaned = pattern.sub('', cleaned)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user afraid short amzn looking like near monopoly ebooks infrastructure service'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = \"user I'd be afraid to short AMZN - and they are looking like a near-monopoly in eBooks and infrastructure-as-a-service\"\n",
    "process_text(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to stem the sentences\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kicker watchlist xide tit soq pnk cpw bpz aj trade method method see prev post'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = 'kickers watchlist xide tit soq pnk cpw bpz aj trade method method see prev posts'\n",
    "stem_text(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kicker watchlist xide tit soq pnk cpw bpz aj t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user aap movi return fea geed indic trade year...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user afraid short amzn look like near monopoli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnta over</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oi over</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  kicker watchlist xide tit soq pnk cpw bpz aj t...          1\n",
       "1  user aap movi return fea geed indic trade year...          1\n",
       "2  user afraid short amzn look like near monopoli...          1\n",
       "3                                          mnta over          1\n",
       "4                                            oi over          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: stem_text(process_text(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>momentum come back etfc broke ma resist solid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>user gameplan shot today but like trend break ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vs invert head shoulder play well wasn abl cat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>red not readi break</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>user bac quick trade late but invest good entr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5779</th>\n",
       "      <td>investor lose rs lakh crore worst day market o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>tc share price jump no layoff dividend announc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>gold price slip below rs investor book profit ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>sharemarket live sensex day high up point nift...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>sensex nifti climb day high still up key facto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "14    momentum come back etfc broke ma resist solid ...          1\n",
       "16    user gameplan shot today but like trend break ...          1\n",
       "25    vs invert head shoulder play well wasn abl cat...          1\n",
       "26                                  red not readi break         -1\n",
       "28    user bac quick trade late but invest good entr...          1\n",
       "...                                                 ...        ...\n",
       "5779  investor lose rs lakh crore worst day market o...         -1\n",
       "5785  tc share price jump no layoff dividend announc...          1\n",
       "5787  gold price slip below rs investor book profit ...         -1\n",
       "5789  sharemarket live sensex day high up point nift...          1\n",
       "5790  sensex nifti climb day high still up key facto...          1\n",
       "\n",
       "[1458 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the exceptions affects the sentiment\n",
    "df.loc[df['Text'].str.contains(' | '.join(exception_words)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = df['Sentiment'].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word freq dict\n",
    "all_words = []\n",
    "\n",
    "for sentence in X:\n",
    "    words = sentence.split()\n",
    "    all_words.extend(words)\n",
    "\n",
    "freq_dist = FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'aap': 929, 'co': 711, 'http': 696, 'user': 648, 'short': 522, 'up': 440, 'day': 385, 'stock': 372, 'over': 352, 'today': 347, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of words with freq <= threshold\n",
    "'''\n",
    "Attempted tweaking the threshold but did not find any significant improvements\n",
    "'''\n",
    "threshold = 0 # threshold for low freq words \n",
    "low_freq_list = []\n",
    "\n",
    "for word, freq in freq_dist.items():\n",
    "    if freq <= threshold: \n",
    "        low_freq_list.append(word)\n",
    "\n",
    "low_freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove words with low freq\n",
    "def remove_low_freq(text):\n",
    "    words = text.split()\n",
    "    removed = [word for word in words if word not in low_freq_list]\n",
    "    return ' '.join(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       kicker watchlist xide tit soq pnk cpw bpz aj t...\n",
       "1       user aap movi return fea geed indic trade year...\n",
       "2       user afraid short amzn look like near monopoli...\n",
       "3                                               mnta over\n",
       "4                                                 oi over\n",
       "                              ...                        \n",
       "5786    industri bodi cii said discom like suffer net ...\n",
       "5787    gold price slip below rs investor book profit ...\n",
       "5788    worker bajaj auto agre wage cut period april t...\n",
       "5789    sharemarket live sensex day high up point nift...\n",
       "5790    sensex nifti climb day high still up key facto...\n",
       "Name: Text, Length: 5791, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.apply(lambda x: remove_low_freq(x))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4632,), (1159,), (4632,), (1159,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80 20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing\n",
    "\n",
    "Convert every sentence into an sequences of indexes which represent the words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Create word to index dictionary\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sentence length is 23\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for i in X_train:\n",
    "    if len(i) > maxlen:\n",
    "        maxlen = len(i)\n",
    "for i in X_test:\n",
    "    if len(i) > maxlen:\n",
    "        maxlen = len(i)\n",
    "print(f'The maximum sentence length is {maxlen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aap': 1,\n",
       " 'co': 2,\n",
       " 'http': 3,\n",
       " 'user': 4,\n",
       " 'short': 5,\n",
       " 'up': 6,\n",
       " 'day': 7,\n",
       " 'stock': 8,\n",
       " 'over': 9,\n",
       " 'like': 10,\n",
       " 'today': 11,\n",
       " 'look': 12,\n",
       " 'volum': 13,\n",
       " 'buy': 14,\n",
       " 'but': 15,\n",
       " 'market': 16,\n",
       " 'long': 17,\n",
       " 'stop': 18,\n",
       " 'watch': 19,\n",
       " 'break': 20,\n",
       " 'trade': 21,\n",
       " 'go': 22,\n",
       " 'move': 23,\n",
       " 'good': 24,\n",
       " 'nice': 25,\n",
       " 'high': 26,\n",
       " 'abov': 27,\n",
       " 'close': 28,\n",
       " 'not': 29,\n",
       " 'new': 30,\n",
       " 'goog': 31,\n",
       " 'still': 32,\n",
       " 'bac': 33,\n",
       " 'back': 34,\n",
       " 'time': 35,\n",
       " 'down': 36,\n",
       " 'see': 37,\n",
       " 'get': 38,\n",
       " 'higher': 39,\n",
       " 'week': 40,\n",
       " 'coronaviru': 41,\n",
       " 'next': 42,\n",
       " 'price': 43,\n",
       " 'hold': 44,\n",
       " 'one': 45,\n",
       " 'posit': 46,\n",
       " 'no': 47,\n",
       " 'ong': 48,\n",
       " 'call': 49,\n",
       " 'triangl': 50,\n",
       " 'point': 51,\n",
       " 'sensex': 52,\n",
       " 'take': 53,\n",
       " 'share': 54,\n",
       " 'weekli': 55,\n",
       " 'sell': 56,\n",
       " 'breakout': 57,\n",
       " 'nifti': 58,\n",
       " 'open': 59,\n",
       " 'support': 60,\n",
       " 'low': 61,\n",
       " 'target': 62,\n",
       " 'p': 63,\n",
       " 'could': 64,\n",
       " 'come': 65,\n",
       " 'big': 66,\n",
       " 'bullish': 67,\n",
       " 'lower': 68,\n",
       " 'gap': 69,\n",
       " 'year': 70,\n",
       " 'nfx': 71,\n",
       " 'last': 72,\n",
       " 'amzn': 73,\n",
       " 'earn': 74,\n",
       " 'f': 75,\n",
       " 'make': 76,\n",
       " 'green': 77,\n",
       " 'put': 78,\n",
       " 'start': 79,\n",
       " 'think': 80,\n",
       " 'say': 81,\n",
       " 'bank': 82,\n",
       " 'rt': 83,\n",
       " 'line': 84,\n",
       " 'below': 85,\n",
       " 'continu': 86,\n",
       " 'may': 87,\n",
       " 'need': 88,\n",
       " 'under': 89,\n",
       " 'c': 90,\n",
       " 'hit': 91,\n",
       " 'profit': 92,\n",
       " 'well': 93,\n",
       " 'flag': 94,\n",
       " 'sinc': 95,\n",
       " 'b': 96,\n",
       " 'strong': 97,\n",
       " 'list': 98,\n",
       " 'ad': 99,\n",
       " 'would': 100,\n",
       " 'amid': 101,\n",
       " 'resist': 102,\n",
       " 'term': 103,\n",
       " 'level': 104,\n",
       " 'cash': 105,\n",
       " 'trend': 106,\n",
       " 'tomorrow': 107,\n",
       " 'k': 108,\n",
       " 'ddd': 109,\n",
       " 'first': 110,\n",
       " 'w': 111,\n",
       " 'daili': 112,\n",
       " 'great': 113,\n",
       " 'ook': 114,\n",
       " 'spi': 115,\n",
       " 'compani': 116,\n",
       " 'u': 117,\n",
       " 'fall': 118,\n",
       " 'full': 119,\n",
       " 'anoth': 120,\n",
       " 'want': 121,\n",
       " 'yesterday': 122,\n",
       " 'entri': 123,\n",
       " 'expect': 124,\n",
       " 'gain': 125,\n",
       " 'post': 126,\n",
       " 'run': 127,\n",
       " 'gs': 128,\n",
       " 'ed': 129,\n",
       " 'ma': 130,\n",
       " 'month': 131,\n",
       " 'option': 132,\n",
       " 'set': 133,\n",
       " 'bounc': 134,\n",
       " 'play': 135,\n",
       " 'top': 136,\n",
       " 'work': 137,\n",
       " 'bottom': 138,\n",
       " 'street': 139,\n",
       " 'end': 140,\n",
       " 'v': 141,\n",
       " 'imo': 142,\n",
       " 'tri': 143,\n",
       " 'march': 144,\n",
       " 'drop': 145,\n",
       " 'q': 146,\n",
       " 'bull': 147,\n",
       " 'oil': 148,\n",
       " 'x': 149,\n",
       " 'soon': 150,\n",
       " 'wait': 151,\n",
       " 'right': 152,\n",
       " 'keep': 153,\n",
       " 'million': 154,\n",
       " 'base': 155,\n",
       " 'around': 156,\n",
       " 'follow': 157,\n",
       " 'pullback': 158,\n",
       " 'money': 159,\n",
       " 'test': 160,\n",
       " 'possibl': 161,\n",
       " 'setup': 162,\n",
       " 'appl': 163,\n",
       " 'bought': 164,\n",
       " 'much': 165,\n",
       " 'pay': 166,\n",
       " 'beat': 167,\n",
       " 'vol': 168,\n",
       " 'head': 169,\n",
       " 'global': 170,\n",
       " 'report': 171,\n",
       " 'interest': 172,\n",
       " 'es': 173,\n",
       " 'cat': 174,\n",
       " 'investor': 175,\n",
       " 'loss': 176,\n",
       " 'also': 177,\n",
       " 'don': 178,\n",
       " 'friday': 179,\n",
       " 'fund': 180,\n",
       " 'mani': 181,\n",
       " 'news': 182,\n",
       " 'znga': 183,\n",
       " 'sma': 184,\n",
       " 'small': 185,\n",
       " 'h': 186,\n",
       " 'show': 187,\n",
       " 'cut': 188,\n",
       " 'macd': 189,\n",
       " 'pattern': 190,\n",
       " 'tgt': 191,\n",
       " 'red': 192,\n",
       " 'shot': 193,\n",
       " 'realli': 194,\n",
       " 'averag': 195,\n",
       " 'spx': 196,\n",
       " 'near': 197,\n",
       " 'crisi': 198,\n",
       " 'even': 199,\n",
       " 'consolid': 200,\n",
       " 'pick': 201,\n",
       " 'littl': 202,\n",
       " 'bearish': 203,\n",
       " 'confirm': 204,\n",
       " 'lot': 205,\n",
       " 'bear': 206,\n",
       " 'rise': 207,\n",
       " 'feb': 208,\n",
       " 'scale': 209,\n",
       " 'action': 210,\n",
       " 'hope': 211,\n",
       " 'min': 212,\n",
       " 'aapl': 213,\n",
       " 'sale': 214,\n",
       " 'fill': 215,\n",
       " 'e': 216,\n",
       " 'vs': 217,\n",
       " 'ssi': 218,\n",
       " 'trader': 219,\n",
       " 'swing': 220,\n",
       " 'sold': 221,\n",
       " 'pdate': 222,\n",
       " 'growth': 223,\n",
       " 'nkd': 224,\n",
       " 'st': 225,\n",
       " 'know': 226,\n",
       " 'upsid': 227,\n",
       " 'us': 228,\n",
       " 'two': 229,\n",
       " 'busi': 230,\n",
       " 'pop': 231,\n",
       " 'jpm': 232,\n",
       " 'ng': 233,\n",
       " 'goe': 234,\n",
       " 'bid': 235,\n",
       " 'de': 236,\n",
       " 'cover': 237,\n",
       " 'si': 238,\n",
       " 'bet': 239,\n",
       " 'invest': 240,\n",
       " 'cross': 241,\n",
       " 'jcp': 242,\n",
       " 'fb': 243,\n",
       " 'heard': 244,\n",
       " 'way': 245,\n",
       " 'morn': 246,\n",
       " 'better': 247,\n",
       " 'pull': 248,\n",
       " 'jan': 249,\n",
       " 'pandem': 250,\n",
       " 'china': 251,\n",
       " 'made': 252,\n",
       " 'broke': 253,\n",
       " 'g': 254,\n",
       " 'might': 255,\n",
       " 'ralli': 256,\n",
       " 'got': 257,\n",
       " 'monthli': 258,\n",
       " 'bbi': 259,\n",
       " 'recent': 260,\n",
       " 'miss': 261,\n",
       " 'fs': 262,\n",
       " 'gd': 263,\n",
       " 'stay': 264,\n",
       " 'turn': 265,\n",
       " 'let': 266,\n",
       " 'love': 267,\n",
       " 'push': 268,\n",
       " 'ema': 269,\n",
       " 'billion': 270,\n",
       " 'weak': 271,\n",
       " 'key': 272,\n",
       " 'imho': 273,\n",
       " 'declin': 274,\n",
       " 'trigger': 275,\n",
       " 'th': 276,\n",
       " 'clear': 277,\n",
       " 'world': 278,\n",
       " 'video': 279,\n",
       " 'area': 280,\n",
       " 'nvda': 281,\n",
       " 'risk': 282,\n",
       " 'oper': 283,\n",
       " 'earli': 284,\n",
       " 'net': 285,\n",
       " 'signal': 286,\n",
       " 'major': 287,\n",
       " 'lockdown': 288,\n",
       " 'cf': 289,\n",
       " 'revers': 290,\n",
       " 'trail': 291,\n",
       " 'govern': 292,\n",
       " 'msft': 293,\n",
       " 'gmc': 294,\n",
       " 'minut': 295,\n",
       " 'financi': 296,\n",
       " 'cost': 297,\n",
       " 'jump': 298,\n",
       " 'mcp': 299,\n",
       " 'huge': 300,\n",
       " 'ave': 301,\n",
       " 'mayb': 302,\n",
       " 'far': 303,\n",
       " 'current': 304,\n",
       " 'readi': 305,\n",
       " 'rate': 306,\n",
       " 'fed': 307,\n",
       " 'thing': 308,\n",
       " 'dollar': 309,\n",
       " 'channel': 310,\n",
       " 'best': 311,\n",
       " 'worst': 312,\n",
       " 'yhoo': 313,\n",
       " 'check': 314,\n",
       " 'idea': 315,\n",
       " 'add': 316,\n",
       " 'via': 317,\n",
       " 'app': 318,\n",
       " 'rbi': 319,\n",
       " 'eye': 320,\n",
       " 'flow': 321,\n",
       " 'give': 322,\n",
       " 'csn': 323,\n",
       " 'intraday': 324,\n",
       " 'peopl': 325,\n",
       " 'help': 326,\n",
       " 'technic': 327,\n",
       " 'job': 328,\n",
       " 'plan': 329,\n",
       " 'candl': 330,\n",
       " 'rs': 331,\n",
       " 'solid': 332,\n",
       " 'hpq': 333,\n",
       " 'rang': 334,\n",
       " 'final': 335,\n",
       " 'lead': 336,\n",
       " 'done': 337,\n",
       " 'r': 338,\n",
       " 'fcx': 339,\n",
       " 'valu': 340,\n",
       " 'chk': 341,\n",
       " 'economi': 342,\n",
       " 'n': 343,\n",
       " 'pt': 344,\n",
       " 'fib': 345,\n",
       " 'uptrend': 346,\n",
       " 'swi': 347,\n",
       " 'fail': 348,\n",
       " 'squeez': 349,\n",
       " 'vng': 350,\n",
       " 'hour': 351,\n",
       " 'bar': 352,\n",
       " 'wall': 353,\n",
       " 'ep': 354,\n",
       " 'dividend': 355,\n",
       " 'indic': 356,\n",
       " 'handl': 357,\n",
       " 'hard': 358,\n",
       " 'late': 359,\n",
       " 'yr': 360,\n",
       " 'data': 361,\n",
       " 'dip': 362,\n",
       " 'away': 363,\n",
       " 'gpn': 364,\n",
       " 'neg': 365,\n",
       " 'use': 366,\n",
       " 'second': 367,\n",
       " 'increas': 368,\n",
       " 'thru': 369,\n",
       " 'ebay': 370,\n",
       " 'updat': 371,\n",
       " 'ish': 372,\n",
       " 'ye': 373,\n",
       " 'surg': 374,\n",
       " 'oi': 375,\n",
       " 'monday': 376,\n",
       " 'fast': 377,\n",
       " 'probabl': 378,\n",
       " 'cup': 379,\n",
       " 'prior': 380,\n",
       " 'hedg': 381,\n",
       " 'includ': 382,\n",
       " 'anyon': 383,\n",
       " 'happen': 384,\n",
       " 'cst': 385,\n",
       " 'record': 386,\n",
       " 'watchlist': 387,\n",
       " 'took': 388,\n",
       " 'ko': 389,\n",
       " 'bit': 390,\n",
       " 'industri': 391,\n",
       " 'mar': 392,\n",
       " 'yet': 393,\n",
       " 'side': 394,\n",
       " 'ibm': 395,\n",
       " 'offer': 396,\n",
       " 'form': 397,\n",
       " 'wow': 398,\n",
       " 'marketupd': 399,\n",
       " 'almost': 400,\n",
       " 'bond': 401,\n",
       " 'home': 402,\n",
       " 'sv': 403,\n",
       " 'till': 404,\n",
       " 'everyon': 405,\n",
       " 'ago': 406,\n",
       " 'gold': 407,\n",
       " 'potenti': 408,\n",
       " 'vxi': 409,\n",
       " 'name': 410,\n",
       " 'hod': 411,\n",
       " 'past': 412,\n",
       " 'cs': 413,\n",
       " 'wk': 414,\n",
       " 'po': 415,\n",
       " 'bad': 416,\n",
       " 'buyer': 417,\n",
       " 'feel': 418,\n",
       " 'rais': 419,\n",
       " 'someth': 420,\n",
       " 'econom': 421,\n",
       " 'volatil': 422,\n",
       " 'said': 423,\n",
       " 'quarter': 424,\n",
       " 'wmt': 425,\n",
       " 'manag': 426,\n",
       " 'trendlin': 427,\n",
       " 'india': 428,\n",
       " 'crore': 429,\n",
       " 'ms': 430,\n",
       " 'fio': 431,\n",
       " 'everi': 432,\n",
       " 'seem': 433,\n",
       " 'mo': 434,\n",
       " 'spw': 435,\n",
       " 'note': 436,\n",
       " 'quick': 437,\n",
       " 'hot': 438,\n",
       " 'shd': 439,\n",
       " 'demand': 440,\n",
       " 'fear': 441,\n",
       " 'doesn': 442,\n",
       " 'sure': 443,\n",
       " 'won': 444,\n",
       " 'heavi': 445,\n",
       " 'pm': 446,\n",
       " 'went': 447,\n",
       " 'per': 448,\n",
       " 'step': 449,\n",
       " 'product': 450,\n",
       " 'avg': 451,\n",
       " 'alreadi': 452,\n",
       " 'biggest': 453,\n",
       " 'ever': 454,\n",
       " 'downsid': 455,\n",
       " 'easi': 456,\n",
       " 'cm': 457,\n",
       " 'alway': 458,\n",
       " 'doubl': 459,\n",
       " 'cheap': 460,\n",
       " 'fire': 461,\n",
       " 'pcn': 462,\n",
       " 'ot': 463,\n",
       " 'insid': 464,\n",
       " 'pretti': 465,\n",
       " 'dndn': 466,\n",
       " 'mean': 467,\n",
       " 'roll': 468,\n",
       " 'care': 469,\n",
       " 'sign': 470,\n",
       " 'wfc': 471,\n",
       " 'gevo': 472,\n",
       " 'consum': 473,\n",
       " 'free': 474,\n",
       " 'futur': 475,\n",
       " 'qcom': 476,\n",
       " 'larg': 477,\n",
       " 'overnight': 478,\n",
       " 'tell': 479,\n",
       " 'seller': 480,\n",
       " 'oc': 481,\n",
       " 'ceo': 482,\n",
       " 'talk': 483,\n",
       " 'deal': 484,\n",
       " 'enter': 485,\n",
       " 'tight': 486,\n",
       " 'mtg': 487,\n",
       " 'shoulder': 488,\n",
       " 'eod': 489,\n",
       " 'dump': 490,\n",
       " 'qqq': 491,\n",
       " 'z': 492,\n",
       " 'spread': 493,\n",
       " 'number': 494,\n",
       " 'slow': 495,\n",
       " 'estim': 496,\n",
       " 'ow': 497,\n",
       " 'invn': 498,\n",
       " 'diverg': 499,\n",
       " 'retest': 500,\n",
       " 'return': 501,\n",
       " 'touch': 502,\n",
       " 'ist': 503,\n",
       " 'lose': 504,\n",
       " 'debt': 505,\n",
       " 'analyst': 506,\n",
       " 'se': 507,\n",
       " 'didn': 508,\n",
       " 'en': 509,\n",
       " 'amp': 510,\n",
       " 'chang': 511,\n",
       " 'real': 512,\n",
       " 'win': 513,\n",
       " 'da': 514,\n",
       " 'april': 515,\n",
       " 'game': 516,\n",
       " 'cee': 517,\n",
       " 'result': 518,\n",
       " 'pre': 519,\n",
       " 'swhc': 520,\n",
       " 'covid': 521,\n",
       " 'dma': 522,\n",
       " 'momentum': 523,\n",
       " 'tc': 524,\n",
       " 'aig': 525,\n",
       " 'rest': 526,\n",
       " 'thank': 527,\n",
       " 'sa': 528,\n",
       " 'worth': 529,\n",
       " 'box': 530,\n",
       " 'iphon': 531,\n",
       " 'cent': 532,\n",
       " 'build': 533,\n",
       " 'never': 534,\n",
       " 'mark': 535,\n",
       " 'apo': 536,\n",
       " 'remain': 537,\n",
       " 'plung': 538,\n",
       " 'loan': 539,\n",
       " 'cap': 540,\n",
       " 'book': 541,\n",
       " 'extend': 542,\n",
       " 'deck': 543,\n",
       " 'due': 544,\n",
       " 'though': 545,\n",
       " 'bvsn': 546,\n",
       " 'correct': 547,\n",
       " 'ike': 548,\n",
       " 'beauti': 549,\n",
       " 'other': 550,\n",
       " 'three': 551,\n",
       " 'downtrend': 552,\n",
       " 'patienc': 553,\n",
       " 'size': 554,\n",
       " 'appear': 555,\n",
       " 'contract': 556,\n",
       " 'mon': 557,\n",
       " 'half': 558,\n",
       " 'inflat': 559,\n",
       " 'read': 560,\n",
       " 'cloud': 561,\n",
       " 'tonight': 562,\n",
       " 'enough': 563,\n",
       " 'ca': 564,\n",
       " 'ana': 565,\n",
       " 'dead': 566,\n",
       " 'lol': 567,\n",
       " 'europ': 568,\n",
       " 'split': 569,\n",
       " 'qco': 570,\n",
       " 'part': 571,\n",
       " 'j': 572,\n",
       " 'firm': 573,\n",
       " 'januari': 574,\n",
       " 'act': 575,\n",
       " 'rip': 576,\n",
       " 'vmw': 577,\n",
       " 'mkt': 578,\n",
       " 'announc': 579,\n",
       " 'straight': 580,\n",
       " 'ga': 581,\n",
       " 'crude': 582,\n",
       " 'trap': 583,\n",
       " 'reason': 584,\n",
       " 'rebound': 585,\n",
       " 'float': 586,\n",
       " 'sk': 587,\n",
       " 'grow': 588,\n",
       " 'accumul': 589,\n",
       " 'dow': 590,\n",
       " 'retail': 591,\n",
       " 'held': 592,\n",
       " 'affi': 593,\n",
       " 'addit': 594,\n",
       " 'power': 595,\n",
       " 'equiti': 596,\n",
       " 'american': 597,\n",
       " 'agre': 598,\n",
       " 'dvax': 599,\n",
       " 'intc': 600,\n",
       " 'thought': 601,\n",
       " 'phone': 602,\n",
       " 'sd': 603,\n",
       " 'xco': 604,\n",
       " 'ft': 605,\n",
       " 'catch': 606,\n",
       " 'largest': 607,\n",
       " 'oversold': 608,\n",
       " 'thursday': 609,\n",
       " 'margin': 610,\n",
       " 'account': 611,\n",
       " 'onlin': 612,\n",
       " 'find': 613,\n",
       " 'leg': 614,\n",
       " 'overbought': 615,\n",
       " 'dec': 616,\n",
       " 'wrong': 617,\n",
       " 'isi': 618,\n",
       " 'despit': 619,\n",
       " 'surpris': 620,\n",
       " 'pphm': 621,\n",
       " 'ec': 622,\n",
       " 'suppli': 623,\n",
       " 'decent': 624,\n",
       " 'analysi': 625,\n",
       " 'creat': 626,\n",
       " 'liquid': 627,\n",
       " 'ha': 628,\n",
       " 'imm': 629,\n",
       " 'ask': 630,\n",
       " 'must': 631,\n",
       " 'pois': 632,\n",
       " 'gmx': 633,\n",
       " 'central': 634,\n",
       " 'suggest': 635,\n",
       " 'yep': 636,\n",
       " 'mgm': 637,\n",
       " 'old': 638,\n",
       " 'ax': 639,\n",
       " 'servic': 640,\n",
       " 'employe': 641,\n",
       " 'ocn': 642,\n",
       " 'ahead': 643,\n",
       " 'sector': 644,\n",
       " 'wynn': 645,\n",
       " 'purchas': 646,\n",
       " 'et': 647,\n",
       " 'countri': 648,\n",
       " 'saudi': 649,\n",
       " 'winner': 650,\n",
       " 'session': 651,\n",
       " 'crash': 652,\n",
       " 'slump': 653,\n",
       " 'bring': 654,\n",
       " 'tsa': 655,\n",
       " 'earlier': 656,\n",
       " 'group': 657,\n",
       " 'stori': 658,\n",
       " 'rupe': 659,\n",
       " 'pin': 660,\n",
       " 'believ': 661,\n",
       " 'ok': 662,\n",
       " 'climb': 663,\n",
       " 'least': 664,\n",
       " 'gdp': 665,\n",
       " 'ceg': 666,\n",
       " 'retrac': 667,\n",
       " 'broken': 668,\n",
       " 'airlin': 669,\n",
       " 'hd': 670,\n",
       " 'sbx': 671,\n",
       " 'wonder': 672,\n",
       " 'approach': 673,\n",
       " 'ipad': 674,\n",
       " 'expir': 675,\n",
       " 'noth': 676,\n",
       " 'nke': 677,\n",
       " 'wrap': 678,\n",
       " 'compq': 679,\n",
       " 'dt': 680,\n",
       " 'zagg': 681,\n",
       " 'sorri': 682,\n",
       " 'nd': 683,\n",
       " 'dia': 684,\n",
       " 'chanc': 685,\n",
       " 'later': 686,\n",
       " 'bt': 687,\n",
       " 'settl': 688,\n",
       " 'hate': 689,\n",
       " 'hammer': 690,\n",
       " 'gdx': 691,\n",
       " 'div': 692,\n",
       " 'lost': 693,\n",
       " 'initi': 694,\n",
       " 'lag': 695,\n",
       " 'csod': 696,\n",
       " 'portfolio': 697,\n",
       " 'corpor': 698,\n",
       " 'everyth': 699,\n",
       " 'place': 700,\n",
       " 'face': 701,\n",
       " 'coh': 702,\n",
       " 'fight': 703,\n",
       " 'samsung': 704,\n",
       " 'pressur': 705,\n",
       " 'begin': 706,\n",
       " 'ove': 707,\n",
       " 'cmg': 708,\n",
       " 'without': 709,\n",
       " 'upward': 710,\n",
       " 'massiv': 711,\n",
       " 'bk': 712,\n",
       " 'mention': 713,\n",
       " 'store': 714,\n",
       " 'googl': 715,\n",
       " 'pivot': 716,\n",
       " 'dnkn': 717,\n",
       " 'kwk': 718,\n",
       " 'downgrad': 719,\n",
       " 'februari': 720,\n",
       " 'ignor': 721,\n",
       " 'consid': 722,\n",
       " 'zone': 723,\n",
       " 'borrow': 724,\n",
       " 'crack': 725,\n",
       " 'gonna': 726,\n",
       " 'health': 727,\n",
       " 'ride': 728,\n",
       " 'state': 729,\n",
       " 'alert': 730,\n",
       " 'ead': 731,\n",
       " 'hf': 732,\n",
       " 'aa': 733,\n",
       " 'matter': 734,\n",
       " 'boost': 735,\n",
       " 'tv': 736,\n",
       " 'toward': 737,\n",
       " 'eas': 738,\n",
       " 'smart': 739,\n",
       " 'mil': 740,\n",
       " 'worri': 741,\n",
       " 'struggl': 742,\n",
       " 'light': 743,\n",
       " 'ay': 744,\n",
       " 'byd': 745,\n",
       " 'allow': 746,\n",
       " 'catalyst': 747,\n",
       " 'revenu': 748,\n",
       " 'oh': 749,\n",
       " 'fundament': 750,\n",
       " 'hand': 751,\n",
       " 'left': 752,\n",
       " 'sit': 753,\n",
       " 'opex': 754,\n",
       " 'someon': 755,\n",
       " 'chase': 756,\n",
       " 'forecast': 757,\n",
       " 'exp': 758,\n",
       " 'guy': 759,\n",
       " 'recess': 760,\n",
       " 'cook': 761,\n",
       " 'radar': 762,\n",
       " 'gm': 763,\n",
       " 'reclaim': 764,\n",
       " 'kbh': 765,\n",
       " 'dk': 766,\n",
       " 'wors': 767,\n",
       " 'issu': 768,\n",
       " 'histori': 769,\n",
       " 'spike': 770,\n",
       " 'coupl': 771,\n",
       " 'impact': 772,\n",
       " 'sound': 773,\n",
       " 'px': 774,\n",
       " 'upgrad': 775,\n",
       " 'governor': 776,\n",
       " 'either': 777,\n",
       " 'blog': 778,\n",
       " 'amazon': 779,\n",
       " 'momo': 780,\n",
       " 'apr': 781,\n",
       " 'hang': 782,\n",
       " 'doji': 783,\n",
       " 'descend': 784,\n",
       " 'boom': 785,\n",
       " 'expand': 786,\n",
       " 'decis': 787,\n",
       " 'whole': 788,\n",
       " 'ba': 789,\n",
       " 'produc': 790,\n",
       " 'ppl': 791,\n",
       " 'crazi': 792,\n",
       " 'premarket': 793,\n",
       " 'feder': 794,\n",
       " 'card': 795,\n",
       " 'solar': 796,\n",
       " 'insight': 797,\n",
       " 'view': 798,\n",
       " 'war': 799,\n",
       " 'slide': 800,\n",
       " 'prefer': 801,\n",
       " 'unless': 802,\n",
       " 'lift': 803,\n",
       " 'wfm': 804,\n",
       " 'folk': 805,\n",
       " 'cen': 806,\n",
       " 'sca': 807,\n",
       " 'nem': 808,\n",
       " 'lakh': 809,\n",
       " 'foreign': 810,\n",
       " 'ex': 811,\n",
       " 'index': 812,\n",
       " 'god': 813,\n",
       " 'vv': 814,\n",
       " 'rd': 815,\n",
       " 'flush': 816,\n",
       " 'capit': 817,\n",
       " 'problem': 818,\n",
       " 'gae': 819,\n",
       " 'south': 820,\n",
       " 'wouldn': 821,\n",
       " 'credit': 822,\n",
       " 'vhc': 823,\n",
       " 'benefit': 824,\n",
       " 'delta': 825,\n",
       " 'shut': 826,\n",
       " 'die': 827,\n",
       " 'wsj': 828,\n",
       " 'isn': 829,\n",
       " 'mako': 830,\n",
       " 'usual': 831,\n",
       " 'perform': 832,\n",
       " 'longer': 833,\n",
       " 'stoploss': 834,\n",
       " 'night': 835,\n",
       " 'auto': 836,\n",
       " 'incom': 837,\n",
       " 'balanc': 838,\n",
       " 'within': 839,\n",
       " 'tap': 840,\n",
       " 'wedg': 841,\n",
       " 'resum': 842,\n",
       " 'cbmx': 843,\n",
       " 'saw': 844,\n",
       " 'block': 845,\n",
       " 'save': 846,\n",
       " 'recov': 847,\n",
       " 'wt': 848,\n",
       " 'seen': 849,\n",
       " 'explain': 850,\n",
       " 'upper': 851,\n",
       " 'ise': 852,\n",
       " 'ns': 853,\n",
       " 'case': 854,\n",
       " 'tax': 855,\n",
       " 'leader': 856,\n",
       " 'af': 857,\n",
       " 'predict': 858,\n",
       " 'fade': 859,\n",
       " 'anybodi': 860,\n",
       " 'csco': 861,\n",
       " 'wen': 862,\n",
       " 'histor': 863,\n",
       " 'mini': 864,\n",
       " 'goldman': 865,\n",
       " 'tripl': 866,\n",
       " 'gener': 867,\n",
       " 'happi': 868,\n",
       " 'improv': 869,\n",
       " 'he': 870,\n",
       " 'man': 871,\n",
       " 'explod': 872,\n",
       " 'alpha': 873,\n",
       " 'complet': 874,\n",
       " 'wave': 875,\n",
       " 'attack': 876,\n",
       " 'along': 877,\n",
       " 'caus': 878,\n",
       " 'trump': 879,\n",
       " 'figur': 880,\n",
       " 'inc': 881,\n",
       " 'opportun': 882,\n",
       " 'zc': 883,\n",
       " 'edg': 884,\n",
       " 'l': 885,\n",
       " 'remind': 886,\n",
       " 'energi': 887,\n",
       " 'ngt': 888,\n",
       " 'acquisit': 889,\n",
       " 'sharehold': 890,\n",
       " 'idcc': 891,\n",
       " 'breakdown': 892,\n",
       " 'fresh': 893,\n",
       " 'wake': 894,\n",
       " 'awesom': 895,\n",
       " 'anyth': 896,\n",
       " 'regul': 897,\n",
       " 'social': 898,\n",
       " 'came': 899,\n",
       " 'heo': 900,\n",
       " 'bot': 901,\n",
       " 'valuat': 902,\n",
       " 'dd': 903,\n",
       " 'spot': 904,\n",
       " 'gme': 905,\n",
       " 'finish': 906,\n",
       " 'model': 907,\n",
       " 'wsjheard': 908,\n",
       " 'across': 909,\n",
       " 'arabia': 910,\n",
       " 'stream': 911,\n",
       " 'quit': 912,\n",
       " 'rememb': 913,\n",
       " 'boy': 914,\n",
       " 'intact': 915,\n",
       " 'payment': 916,\n",
       " 'technolog': 917,\n",
       " 'exit': 918,\n",
       " 'strike': 919,\n",
       " 'natur': 920,\n",
       " 'nx': 921,\n",
       " 'mortgag': 922,\n",
       " 'panic': 923,\n",
       " 'basi': 924,\n",
       " 'dn': 925,\n",
       " 'ascend': 926,\n",
       " 'distribut': 927,\n",
       " 'exactli': 928,\n",
       " 'cog': 929,\n",
       " 'less': 930,\n",
       " 'insur': 931,\n",
       " 'yield': 932,\n",
       " 'softwar': 933,\n",
       " 'studi': 934,\n",
       " 'netflix': 935,\n",
       " 'ctic': 936,\n",
       " 'guess': 937,\n",
       " 'fli': 938,\n",
       " 'measur': 939,\n",
       " 'russia': 940,\n",
       " 'monitor': 941,\n",
       " 'stake': 942,\n",
       " 'nearli': 943,\n",
       " 'tape': 944,\n",
       " 'damag': 945,\n",
       " 'peel': 946,\n",
       " 'mm': 947,\n",
       " 'chines': 948,\n",
       " 'ndx': 949,\n",
       " 'actual': 950,\n",
       " 'gp': 951,\n",
       " 'drive': 952,\n",
       " 'print': 953,\n",
       " 'biotech': 954,\n",
       " 'caught': 955,\n",
       " 'ah': 956,\n",
       " 'babi': 957,\n",
       " 'previou': 958,\n",
       " 'rule': 959,\n",
       " 'patient': 960,\n",
       " 'im': 961,\n",
       " 'premium': 962,\n",
       " 'anticip': 963,\n",
       " 'activ': 964,\n",
       " 'csx': 965,\n",
       " 'depo': 966,\n",
       " 'releas': 967,\n",
       " 'di': 968,\n",
       " 'perfect': 969,\n",
       " 'enoc': 970,\n",
       " 'els': 971,\n",
       " 'anad': 972,\n",
       " 'tso': 973,\n",
       " 'spend': 974,\n",
       " 'wi': 975,\n",
       " 'dcth': 976,\n",
       " 'presid': 977,\n",
       " 'cautiou': 978,\n",
       " 'overal': 979,\n",
       " 'latest': 980,\n",
       " 'slowli': 981,\n",
       " 'mobil': 982,\n",
       " 'eadi': 983,\n",
       " 'haven': 984,\n",
       " 'progress': 985,\n",
       " 'gone': 986,\n",
       " 'success': 987,\n",
       " 'launch': 988,\n",
       " 'tek': 989,\n",
       " 'becom': 990,\n",
       " 'weekend': 991,\n",
       " 'wish': 992,\n",
       " 'emerg': 993,\n",
       " 'reserv': 994,\n",
       " 'switch': 995,\n",
       " 'cnbc': 996,\n",
       " 'ur': 997,\n",
       " 'led': 998,\n",
       " 'confer': 999,\n",
       " 'period': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6331"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of unique words\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill sentences with len < maxlen with 0s\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4632, 23), (1159, 23))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape = (num of rows, maxlen)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "An LSTM is used as the words are treated as 'timesteps' where each word affects subsequent inputs.\n",
    "\n",
    "Avoided overfitting by reducing epochs and layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARS\n",
    "EPOCHS = 4 #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 22:24:34.019982: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-12-15 22:24:34.020007: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-12-15 22:24:34.020013: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-12-15 22:24:34.020058: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-15 22:24:34.020088: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 32)            202592    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 227489 (888.63 KB)\n",
      "Trainable params: 227489 (888.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = keras.Sequential([\n",
    "\n",
    "    keras.layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=maxlen),\n",
    "    keras.layers.LSTM(units=64, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 22:24:34.938974: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 83s 704ms/step - loss: 0.6468 - accuracy: 0.6351 - val_loss: 0.5469 - val_accuracy: 0.7163\n",
      "Epoch 2/4\n",
      "116/116 [==============================] - 80s 689ms/step - loss: 0.4446 - accuracy: 0.8219 - val_loss: 0.5212 - val_accuracy: 0.7735\n",
      "Epoch 3/4\n",
      "116/116 [==============================] - 83s 715ms/step - loss: 0.2604 - accuracy: 0.8907 - val_loss: 0.6118 - val_accuracy: 0.7551\n",
      "Epoch 4/4\n",
      "116/116 [==============================] - 80s 691ms/step - loss: 0.1825 - accuracy: 0.9244 - val_loss: 0.7705 - val_accuracy: 0.7702\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 2s 64ms/step - loss: 0.5824 - accuracy: 0.7817\n",
      "Test accuracy: 0.7817083597183228\n"
     ]
    }
   ],
   "source": [
    "test_eval = model.evaluate(X_test, y_test)\n",
    "test_loss, test_acc = test_eval[0], test_eval[1]\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('final.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6332, 50)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GloVe Embeddings\n",
    "embeddings_dict = dict()\n",
    "glove_file = open('glove_files/glove.6B.50d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = vector\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 23, 50)            316600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                29440     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346105 (1.32 MB)\n",
      "Trainable params: 346105 (1.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model = keras.Sequential([\n",
    "\n",
    "    keras.layers.Embedding(input_dim=vocab_size, output_dim=50, weights=[embedding_matrix], input_length=maxlen), # GloVe embeddings\n",
    "    keras.layers.LSTM(units=64, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "glove_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "116/116 [==============================] - 86s 730ms/step - loss: 0.6546 - accuracy: 0.6372 - val_loss: 0.6351 - val_accuracy: 0.6246\n",
      "Epoch 2/4\n",
      "116/116 [==============================] - 81s 700ms/step - loss: 0.6009 - accuracy: 0.6475 - val_loss: 0.5578 - val_accuracy: 0.6785\n",
      "Epoch 3/4\n",
      "116/116 [==============================] - 82s 705ms/step - loss: 0.4939 - accuracy: 0.7657 - val_loss: 0.6001 - val_accuracy: 0.7400\n",
      "Epoch 4/4\n",
      "116/116 [==============================] - 81s 701ms/step - loss: 0.4100 - accuracy: 0.8297 - val_loss: 0.5413 - val_accuracy: 0.7584\n"
     ]
    }
   ],
   "source": [
    "history = glove_model.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 2s 65ms/step - loss: 0.5021 - accuracy: 0.7774\n",
      "Test accuracy: 0.7773942947387695\n"
     ]
    }
   ],
   "source": [
    "glove_test_eval = glove_model.evaluate(X_test, y_test)\n",
    "glove_test_loss, glove_test_acc = glove_test_eval[0], glove_test_eval[1]\n",
    "print(f'Test accuracy: {glove_test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
